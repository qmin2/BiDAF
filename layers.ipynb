{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a90e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'a' (int)\n",
      "Stored variables and their in-db values:\n",
      "a             -> 4\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as Fb\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.legacy import data, datasets\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "a=4\n",
    "%store a\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd413b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1442255176.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\82107\\AppData\\Local\\Temp\\ipykernel_22836\\1442255176.py\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    self.CharEmbedding = torch.nn.Sequential(\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class BIDAF(nn.Module):\n",
    "    def __init__(self, pContext, pQuery,pVocabs):\n",
    "        super(BIDAF, self).__init__()\n",
    "        \n",
    "        self.context = pContext\n",
    "        self.query = pQuery\n",
    "        self.vocabs = pVocabs\n",
    "        #train set\n",
    "        TEXT1 = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "        LABEL1 = data.Field(sequential=False, batch_first=True)\n",
    "        TEXT1.build_vocab(self.vocabs, vectors=GloVe(name='6B', dim=100))\n",
    "        LABEL1.build_vocab(self.vocabs)\n",
    "        embedding_layer1 = nn.Embedding.from_pretrained(TEXT1.vocab.vectors, freeze=False)\n",
    "\n",
    "        \n",
    "        #charEmbedding Layer\n",
    "        self.CharEmbedding = torch.nn.Sequential(\n",
    "            nn.Conv1d(100, 100, (100,5)), #각기다른 필터가 각각의 출력값을 만들어내기를 희망\n",
    "            torch.nn.Maxpool1d(Kernel_size=1))\n",
    "        \n",
    "        #Highway Layer\n",
    "        self.t= nn.Sequential(Linear(hidden_size, hidden_size), nn.Sigmoid())  \n",
    "        self.transformed = nn.Sequential(Linear(hidden_size, hidden_size),nn.ReLU())\n",
    "        #그러네,, hiddensize는 dynamic하게 정해지는거 아닌가?, 어차피 shape을 아니까 그냥 내가 상수를 때려박아도 되는건가?\n",
    "        \n",
    "        #Contextual Layer\n",
    "        self.context_lstm = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "        \n",
    "        #Attention Layer\n",
    "        \n",
    "        #Modeling Layer\n",
    "        self.modeling_lstm= nn.LSTM(input_size, hidden_size, num_layers=2, bidirectional=True)\n",
    "        \n",
    "        #OutputLayer\n",
    "        self.out_start = nn.Linear(input_size, num_of_tokens)\n",
    "        self.out_end = nn.LSTM(input_size, num_of_tokens)\n",
    "        self.softmax= nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #def forward(self, char1, glove1, char2, glove2):  #1은 context, 2는 query\n",
    "    def forward(self, context, query):\n",
    "        \n",
    "        \n",
    "        #input for context\n",
    "        #char\n",
    "        char_output1 = self.CharEmbedding(char1)\n",
    "        \n",
    "        #highway\n",
    "        highway_input1 = torch.concat([char_output1, glove1]) #순서 상관있음??\n",
    "        transformed_output1 = self.transforemd(highway_input1)\n",
    "        t_ratio1 = self.t(highway_input1)\n",
    "        highway_output1 = t_ratio1 * transformed_output1 + (1-t_ratio) * highway_input1\n",
    "        \n",
    "        \n",
    "        #input for query\n",
    "        #char\n",
    "        char_output2 = self.CharEmbedding(char2)\n",
    "        \n",
    "        #highway\n",
    "        highway_input2 = torch.concat([char_output2, glove2]) #순서 상관있음??\n",
    "        transformed_output2 = self.transforemd(highway_input2)\n",
    "        t_ratio2 = self.t(highway_input2)\n",
    "        highway_output2 = t_ratio2 * transformed_output2 + (1-t_ratio) * highway_input2\n",
    "        \n",
    "        \n",
    "        #contextual\n",
    "        context_output, hidden = self.context_lstm(highway_output, hidden)\n",
    "        LTR_order = context_output[:,0]\n",
    "        RTL_order = context_output[:,1]\n",
    "        context_output= torch.concat([LTR_order, RTL_order])\n",
    "        \n",
    "        #Attention\n",
    "        \n",
    "        attention_output = hidden #matrix g\n",
    "        \n",
    "        #modeling\n",
    "        modeling_output, hidden = self.modeling_lstm(attention_output, hidden)\n",
    "        LTR_order = modeling_output[:,0]\n",
    "        RTL_order = modeling_output[:,1]\n",
    "        modeling_output= torch.concat([LTR_order, RTL_order])\n",
    "        \n",
    "        #output\n",
    "        dense = self.out_start(input)\n",
    "        start= softmax(dense)\n",
    "        #그냥 이렇게하는게 아니라 서로 concat하는 과정 추가해줘야함\n",
    "        lstm, trash = out_end(start, num_of_tokens)\n",
    "        end = softmax(lstm)\n",
    "        \n",
    "        return start, end\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2f114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e7037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b7af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de938b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638937f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e982b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5233777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 1D filters(kernels), each with a width of 5 ,,,,, glove dimension이 100이었으니까! \n",
    "#channel이 100개라고 생각해도 되는걸까? ,,,bias는 false?\n",
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        self.p1 = torch.nn.Sequential(\n",
    "            nn.Conv1d(100, 100, (100,5)), #각기다른 필터가 각각의 출력값을 만들어내기를 희망\n",
    "            torch.nn.Maxpool1d(Kernel_size=1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "            out= self.p1(x)\n",
    "            return out\n",
    "\n",
    "    #누가봐도 이거아닌데 일단 넘어가자\n",
    "    #정답이라면 out은 100d 여야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa97327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ContextLayer, self).__init__()\n",
    "        self.input_size= input_size\n",
    "        self.hidden_size =hidden_size\n",
    "        self.context_lstm = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "        #self.embedding = nn.Embedding(input_size, hidden_size) #이게 있어야 되나??,\n",
    "        #아무리생각해도 embedding 없어도 될듯함. input자체에서 이미 embedding을 받고 오는데\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #embedded= self.embedding(input).view(1,1,-1)\n",
    "        #output= embedded #([[forward1, backward1], [forward2, backward2]]) shape := [n_length, 2]\n",
    "        output = input\n",
    "        output, hidden = self.LSTM(output, hidden)\n",
    "        LTR_order = output[:,0]\n",
    "        RTL_order = output[:,1]\n",
    "        return LTR_order, RTL_order #hidden은 굳이 return 할 필요가 없을 듯\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device=device) #꼭 0으로 초기화 해야되나?\n",
    "    #  그리고 꼭 초기화를해야되나?? 없으면 자동 초기화인가?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fded7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(HighwayLayer, self).__init__()\n",
    "        self.input_size= input_size\n",
    "        self.hidden_size =hidden_size\n",
    "        \n",
    "        self.t= nn.Sequential(Linear(hidden_size, hidden_size), nn.Sigmoid())\n",
    "        self.transformed = nn.Sequential(Linear(hidden_size, hidden_size),nn.ReLU())\n",
    "\n",
    "    def forward(self, glove, char):\n",
    "        \n",
    "        input = torch.concat([char, glove]) #순서 상관있음??\n",
    "        transformed_input = transforemd(input)\n",
    "        t_ratio = t(input)\n",
    "        \n",
    "        output = t_ratio * transformed_input + (1-t_ratio) * input\n",
    "        return output\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30345297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망으로써는, 알파 fun 과 베타 fun의 linear layer인듯??  ,but beta function 은 자유인듯 \n",
    "class AttentionLayer(nn.Module): \n",
    "    def __init__(self, input_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.input_size=  input_size\n",
    "        self.alpha = nn.Linear(input_size, 1) #통과하면 scalar이고, 이걸통해서 similar matrix가되는데.\n",
    "        #그렇다면 matrix로 받아서 한번에 S를 만들어야되는데 =>아닌듯\n",
    "    def forward(self, input):\n",
    "        out= self.alpha(input)\n",
    "        return out\n",
    "    \n",
    "    #def initHidden(self):#이것도 있어야되나?\n",
    "        \n",
    "        \n",
    "    \n",
    "#layer는 class일뿐,,큰틀만 짓고 구체적인 코드는 python으로 구현.\n",
    "#하지만 내부 attention다 클래스내부에서 구현을 해야함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34c7753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ModelingLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.modeling_lstm= nn.LSTM(input_size, hidden_size, num_layers=2, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output= input\n",
    "        output, hidden = self.LSTM(output, hidden)\n",
    "        LTR_order = output[:,0]\n",
    "        RTL_order = output[:,1]\n",
    "        return LTR_order, RTL_order \n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device=device) #꼭 0으로 초기화 해야되나?\n",
    "    #  그리고 꼭 초기화를해야되나?? 없으면 자동 초기화인가?\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b131da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p1과 p2 학습 파라미터\n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, input_size, num_of_tokens): #num_of_tokens = context의 단어(토큰) 수\n",
    "        super(ModelingLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.out_start = nn.Linear(input_size, num_of_tokens)\n",
    "        self.out_end = nn.LSTM(input_size, num_of_tokens)\n",
    "        self.softmax= nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input):#여기 input은 어디서 들어오는거지?\n",
    "        dense = out_start(input)\n",
    "        start= softmax(dense)\n",
    "        lstm, trash=  out_end(start, num_of_tokens)\n",
    "        end = softmax(lstm)\n",
    "        \n",
    "        return start, end\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34d69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56bd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbe8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093509ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf0619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIDAF",
   "language": "python",
   "name": "bidaf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
